# -*- coding: utf-8 -*-
"""Sentiment Classification Using Pretrained and Fine-Tuned BERT-tiny Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mt7P7Ur3ZsYqloVygZnKC4o5WUKb2vTJ
"""

!pip install -q "transformers" "datasets" "evaluate" "gradio"

import os
os.environ["WANDB_DISABLED"] = "true"   # no wandb popup

import numpy as np
import pandas as pd
import torch

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    TextClassificationPipeline,
)
import evaluate
import gradio as gr

print("Libraries imported. Using Transformers, Datasets, Evaluate, Gradio.")

raw_datasets = load_dataset("imdb")
print("Loaded IMDb dataset splits:", raw_datasets.keys())

train_size = 1000
test_size  = 200

small_train = raw_datasets["train"].shuffle(seed=42).select(range(train_size))
small_test  = raw_datasets["test"].shuffle(seed=42).select(range(test_size))

print(f"Training examples: {small_train.num_rows}")
print(f"Test examples: {small_test.num_rows}")

model_name = "prajjwal1/bert-tiny"   # base model for both versions

print("Base model for this experiment:", model_name)

tokenizer = AutoTokenizer.from_pretrained(model_name)
print("Tokenizer loaded for:", model_name)

def tokenize_function(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

print("Tokenizing training data...")
tokenized_train = small_train.map(tokenize_function, batched=True)

print("Tokenizing test data...")
tokenized_test  = small_test.map(tokenize_function, batched=True)

tokenized_train = tokenized_train.remove_columns(["text"]).with_format("torch")
tokenized_test  = tokenized_test.remove_columns(["text"]).with_format("torch")

print("Tokenization complete. Columns now:", tokenized_train.column_names)

accuracy_metric  = evaluate.load("accuracy")
precision_metric = evaluate.load("precision")
recall_metric    = evaluate.load("recall")
f1_metric        = evaluate.load("f1")

print("Loaded metrics: accuracy, precision, recall, f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    result = {}
    result.update(accuracy_metric.compute(predictions=preds, references=labels))
    result.update(precision_metric.compute(predictions=preds, references=labels, average="binary"))
    result.update(recall_metric.compute(predictions=preds, references=labels, average="binary"))
    result.update(f1_metric.compute(predictions=preds, references=labels, average="binary"))
    return result

print("Loading pretrained classification model (Model 1: bert_tiny_pretrained)...")
model_pre = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

eval_args = TrainingArguments(
    output_dir="tmp_eval",
    per_device_eval_batch_size=8,
    do_train=False,
    do_eval=True,
    logging_steps=10,
    report_to="none"
)

trainer_pre = Trainer(
    model=model_pre,
    args=eval_args,
    eval_dataset=tokenized_test,
    compute_metrics=compute_metrics
)

print("Evaluating Model 1 (bert_tiny_pretrained) on test subset...")
metrics_before = trainer_pre.evaluate()
print("Model 1 metrics:", metrics_before)

print("Loading model for fine-tuning (Model 2: bert_tiny_finetuned)...")
model_ft = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

train_args = TrainingArguments(
    output_dir="bert_tiny_imdb_ft",
    eval_strategy="no",          # or evaluation_strategy="no" on older transformers
    save_strategy="no",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    num_train_epochs=0.3,
    max_steps=40,                # limit total steps for speed
    logging_steps=10,
    report_to="none"
)

trainer_ft = Trainer(
    model=model_ft,
    args=train_args,
    train_dataset=tokenized_train,
    compute_metrics=compute_metrics
)

print("Starting training for Model 2 (bert_tiny_finetuned)...")
trainer_ft.train()
print("Training finished. Evaluating Model 2 on test subset...")
metrics_after = trainer_ft.evaluate(eval_dataset=tokenized_test)
print("Model 2 metrics:", metrics_after)

df_before_after = pd.DataFrame([
    {"model": "bert_tiny_pretrained", **metrics_before},
    {"model": "bert_tiny_finetuned", **metrics_after}
])

print("Comparison of models (pretrained vs finetuned):")
display(df_before_after[["model", "eval_accuracy", "eval_precision", "eval_recall", "eval_f1"]])

print("Building inference pipelines for custom sentence testing...")
pipe_pre = TextClassificationPipeline(
    model=model_pre,
    tokenizer=tokenizer,
    return_all_scores=True,
    device=0 if torch.cuda.is_available() else -1
)

pipe_ft = TextClassificationPipeline(
    model=model_ft,
    tokenizer=tokenizer,
    return_all_scores=True,
    device=0 if torch.cuda.is_available() else -1
)

custom_texts = [
    "This movie was absolutely fantastic, I loved every minute.",
    "The film was boring and way too long.",
    "Great acting but the story was weak.",
    "Terrible script and horrible direction.",
    "Not bad, but I expected more."
]

rows = []
for i, text in enumerate(custom_texts):
    for model_label, pipe in [("bert_tiny_pretrained", pipe_pre), ("bert_tiny_finetuned", pipe_ft)]:
        scores = pipe(text)[0]
        pos_score = [s["score"] for s in scores if s["label"] in ["LABEL_1", "POSITIVE"]][0]
        neg_score = [s["score"] for s in scores if s["label"] in ["LABEL_0", "NEGATIVE"]][0]
        pred_label = "positive" if pos_score >= neg_score else "negative"
        rows.append({
            "sample_id": i,
            "text": text[:60] + ("..." if len(text) > 60 else ""),
            "model": model_label,
            "pred_label": pred_label,
            "pos_conf": round(float(pos_score), 4),
            "neg_conf": round(float(neg_score), 4),
        })

df_custom = pd.DataFrame(rows)
print("Custom sentence predictions for both models:")
display(df_custom)

def classify_review(text, model_choice):
    if model_choice == "bert_tiny_finetuned":
        pipe = pipe_ft
    else:
        pipe = pipe_pre

    scores = pipe(text)[0]
    pos_score = [s["score"] for s in scores if s["label"] in ["LABEL_1", "POSITIVE"]][0]
    neg_score = [s["score"] for s in scores if s["label"] in ["LABEL_0", "NEGATIVE"]][0]
    pred_label = "positive" if pos_score >= neg_score else "negative"
    return {
        "chosen_model": model_choice,
        "label": pred_label,
        "positive_confidence": float(pos_score),
        "negative_confidence": float(neg_score)
    }

print("Launching Gradio UI. You can choose between:")
print(" - bert_tiny_pretrained")
print(" - bert_tiny_finetuned")

demo = gr.Interface(
    fn=classify_review,
    inputs=[
        gr.Textbox(lines=3, label="Enter movie review"),
        gr.Dropdown(
            choices=["bert_tiny_pretrained", "bert_tiny_finetuned"],
            value="bert_tiny_finetuned",
            label="Model"
        )
    ],
    outputs="json",
    title="IMDb Sentiment Demo (bert-tiny models)"
)

demo.launch()